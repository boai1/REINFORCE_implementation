{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gPYmtaVt-0t2"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "import math\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "j7pdViC0Cu6C"
   },
   "outputs": [],
   "source": [
    "def smooth(y, window, poly=1):\n",
    "    '''\n",
    "    y: vector to be smoothed \n",
    "    window: size of the smoothing window '''\n",
    "    return savgol_filter(y,window,poly)\n",
    "\n",
    "class LearningCurvePlot:\n",
    "\n",
    "    def __init__(self,title=None):\n",
    "        self.fig,self.ax = plt.subplots()\n",
    "        self.ax.set_xlabel('Episode')\n",
    "        self.ax.set_ylabel('Reward')      \n",
    "        if title is not None:\n",
    "            self.ax.set_title(title)\n",
    "        \n",
    "    def add_curve(self,y,label=None):\n",
    "        ''' y: vector of average reward results\n",
    "        label: string to appear as label in plot legend '''\n",
    "        if label is not None:\n",
    "            self.ax.plot(y,label=label)\n",
    "        else:\n",
    "            self.ax.plot(y)\n",
    "    \n",
    "    def set_ylim(self,lower,upper):\n",
    "        self.ax.set_ylim([lower,upper])\n",
    "\n",
    "    def add_hline(self,height,label):\n",
    "        self.ax.axhline(height,ls='--',c='k',label=label)\n",
    "\n",
    "    def save(self,name='test.png'):\n",
    "        ''' name: string for filename of saved figure '''\n",
    "        self.ax.legend()\n",
    "        self.fig.savefig(name,dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMhSnUV9grGo"
   },
   "source": [
    "## Reinforce Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Fli0yqz9plw4"
   },
   "outputs": [],
   "source": [
    "class ReinforceAgent():\n",
    "\n",
    "  def __init__(self, net, learning_rate, gamma):\n",
    "    self.net = net\n",
    "    self.learning_rate = learning_rate\n",
    "    self.gamma = gamma\n",
    "\n",
    "  def train(self, n_episodes):\n",
    "    print(\" ================== START OF EXPERIMENT ==================\")\n",
    "    \n",
    "    # create the environment and define a random seed for reproducible results\n",
    "    env = gym.make('CartPole-v1')\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # get the number of states and number of actions available for the selected environment\n",
    "    n_states = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n \n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "    \n",
    "    # Create a variable to keep track of the results for each episode\n",
    "    episode_stats = {\n",
    "        'episode': [],\n",
    "        'episode_overall_reward': [],\n",
    "    }\n",
    "    \n",
    "    for ep in range(n_episodes):    \n",
    "        s = env.reset()\n",
    "        # reshape the state vector, s\n",
    "        s = s[None,:]\n",
    "\n",
    "        states = []\n",
    "        rewards = []\n",
    "        actions = []\n",
    "        \n",
    "        done = False\n",
    "\n",
    "        overall_score = 0\n",
    "\n",
    "        #collect trace\n",
    "        while not done:\n",
    "            \n",
    "            # get the probabilities of actions using softmax\n",
    "            probabilities = self.net.predict(s)\n",
    "\n",
    "            # select an action based on the previously computed probabilities\n",
    "            a = np.random.choice(n_actions, p = probabilities[0]) \n",
    "\n",
    "            s_next, r, done, _ = env.step(a)\n",
    "            s_next = s_next[None,:]\n",
    "    \n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            rewards.append(r)\n",
    "\n",
    "            overall_score += r\n",
    "\n",
    "            # update current state\n",
    "            s = s_next\n",
    "\n",
    "            if done:                \n",
    "                break\n",
    "                \n",
    "        # update the weights after the end of each episode\n",
    "        R_total = 0\n",
    "        G_rewards = []\n",
    "        for r in rewards[::-1]:\n",
    "            R_total = r + (self.gamma * R_total)\n",
    "            G_rewards.append(R_total)\n",
    "        G_rewards.reverse()\n",
    "        G_rewards -= np.mean(G_rewards)\n",
    "        G_rewards /= max(G_rewards)        \n",
    "        \n",
    "        grad = 0\n",
    "        \n",
    "        for value in zip(states, actions, G_rewards):\n",
    "            with tf.GradientTape() as tape:\n",
    "              predicted = self.net(value[0])\n",
    "              grad = -value[2] * tfp.distributions.Categorical(probs=predicted, dtype=tf.float32).log_prob(value[1])\n",
    "            loss = tape.gradient(grad,self.net.trainable_variables)\n",
    "            #print(grad)\n",
    "            optimizer.apply_gradients(zip(loss, self.net.trainable_variables))              \n",
    "\n",
    "        print(\"Episode: \"+str(ep)+\" Score: \"+str(overall_score))      \n",
    "        episode_stats['episode'].append(ep)\n",
    "        episode_stats['episode_overall_reward'].append(overall_score)    \n",
    "    \n",
    "    print(\" ================== END OF EXPERIMENT ==================\")\n",
    "    return episode_stats    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MK_udkbaOSAB"
   },
   "source": [
    "## Model Architecture Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5Gj_4XqwOSAC"
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Model with no hidden layers\n",
    "model1 = tf.keras.Sequential()\n",
    "model1.add(tf.keras.layers.Dense(n_actions, activation='softmax', input_shape=(None,n_states)))\n",
    "\n",
    "# Model with low number of neurons on the hidden layers, fully connected\n",
    "model2 = tf.keras.Sequential()\n",
    "model2.add(tf.keras.layers.Dense(4, activation = \"relu\", input_shape=(None,n_states)))\n",
    "model2.add(tf.keras.layers.Dense(4, activation = \"relu\"))\n",
    "model2.add(tf.keras.layers.Dense(n_actions, activation='softmax'))\n",
    "\n",
    "# Model with high number of neurons on the hidden layers, fully connected\n",
    "model3 = tf.keras.Sequential()\n",
    "model3.add(tf.keras.layers.Dense(32, activation = \"relu\", input_shape=(None,n_states)))\n",
    "model3.add(tf.keras.layers.Dense(32, activation = \"relu\"))\n",
    "model3.add(tf.keras.layers.Dense(n_actions, activation='softmax'))\n",
    "\n",
    "# Model 4\n",
    "model4 = tf.keras.Sequential()\n",
    "model4.add(tf.keras.layers.Dense(64, activation = \"relu\", input_shape=(None,n_states)))\n",
    "model4.add(tf.keras.layers.Dense(32, activation = \"relu\"))\n",
    "model4.add(tf.keras.layers.Dense(n_actions, activation='softmax'))\n",
    "          \n",
    "\n",
    "models = {'model1': model1, 'model2': model2, 'model3': model3, 'model4':model4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-2mBfm12Cw36",
    "outputId": "5231638a-d391-418a-b13c-513e0155bd30"
   },
   "outputs": [],
   "source": [
    "plot_title = 'Experimentation with different network architectures' \n",
    "    \n",
    "Plot = LearningCurvePlot(title = plot_title)\n",
    "\n",
    "n_repetitions = 5\n",
    "smoothing_window = 5\n",
    "\n",
    "n_episodes = 200\n",
    "\n",
    "gamma = 0.9\n",
    "lr = 0.001\n",
    "\n",
    "for key in models.keys():\n",
    "    print(f' ============================= START OF {key} =============================')\n",
    "    current_model = models[key]\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for rep in range(n_repetitions):\n",
    "        agent = ReinforceAgent(tf.keras.models.clone_model(current_model), lr, gamma)\n",
    "        exp_stats = agent.train(n_episodes)\n",
    "        results.append(exp_stats['episode_overall_reward'])\n",
    "\n",
    "    # average over repetitions\n",
    "    results_array = np.array(results)\n",
    "    results_averaged = np.mean(results_array, axis=0)\n",
    "\n",
    "    # further smoothing\n",
    "    results_smoothed = smooth(results_averaged,smoothing_window)\n",
    "\n",
    "    plot_label = key \n",
    "    Plot.add_curve(results_smoothed,label=plot_label)\n",
    "\n",
    "\n",
    "Plot.save('A3_model_architecture_plots')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUUCmhoSOSAH"
   },
   "source": [
    "### Experimentation with the number of training episodes\n",
    "\n",
    "Take the best 2 model architectures from th eprevious experiment and train them for a longer period of time to see which one performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mIRQJePIOSAI",
    "outputId": "f23ec4e1-886f-41b9-97ba-bc61f5cb8695"
   },
   "outputs": [],
   "source": [
    "plot_title = 'Experimentation for longer training periods' \n",
    "    \n",
    "Plot = LearningCurvePlot(title = plot_title)\n",
    "\n",
    "n_repetitions = 3\n",
    "smoothing_window = 5\n",
    "\n",
    "n_episodes = 1000\n",
    "\n",
    "gamma = 0.9\n",
    "lr = 0.001\n",
    "\n",
    "models_2 = {'model1': model1, 'model2': model2}\n",
    "\n",
    "for key in models_2.keys():\n",
    "    print(f' ============================= START OF {key} =============================')\n",
    "    current_model = models_2[key]\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for rep in range(n_repetitions):\n",
    "        agent = ReinforceAgent(tf.keras.models.clone_model(current_model), lr, gamma)\n",
    "        exp_stats = agent.train(n_episodes)\n",
    "        results.append(exp_stats['episode_overall_reward'])\n",
    "\n",
    "    # average over repetitions\n",
    "    results_array = np.array(results)\n",
    "    results_averaged = np.mean(results_array, axis=0)\n",
    "\n",
    "    # further smoothing\n",
    "    results_smoothed = smooth(results_averaged,smoothing_window)\n",
    "\n",
    "    plot_label = key \n",
    "    Plot.add_curve(results_smoothed,label=plot_label)\n",
    "\n",
    "\n",
    "Plot.save('A3_model_architecture_plots_longer_train_period')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTOxTGZ0WHp9"
   },
   "source": [
    "### Experimenting with different gamma values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "O3YQZlC5OSAJ",
    "outputId": "c3fba8f9-9589-4098-9f77-1c18c368b28c"
   },
   "outputs": [],
   "source": [
    "n_repetitions = 5\n",
    "smoothing_window = 5\n",
    "\n",
    "n_episodes = 1000\n",
    "\n",
    "gamma_test = [0.3, 0.6, 0.9]\n",
    "learning_rate_test = [0.001, 0.01, 0.1]\n",
    "\n",
    "plot_title = 'Experimentation with different Discount Factors'     \n",
    "Plot = LearningCurvePlot(title = plot_title)\n",
    "\n",
    "for gamma in gamma_test:\n",
    "    print(f' ============================= START OF {gamma} =============================')    \n",
    "    results = []\n",
    "    for rep in range(n_repetitions):\n",
    "        agent = ReinforceAgent(tf.keras.models.clone_model(model1), 0.001, gamma)\n",
    "        exp_stats = agent.train(n_episodes)\n",
    "        results.append(exp_stats['episode_overall_reward'])\n",
    "    # average over repetitions\n",
    "    results_array = np.array(results)\n",
    "    results_averaged = np.mean(results_array, axis=0)\n",
    "    # further smoothing\n",
    "    results_smoothed = smooth(results_averaged,smoothing_window)\n",
    "    plot_label = gamma \n",
    "    Plot.add_curve(results_smoothed,label=plot_label)\n",
    "Plot.save('A3_gamma_plots')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with different learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_title = 'Experimentation with different Learning Rates'     \n",
    "Plot = LearningCurvePlot(title = plot_title)\n",
    "\n",
    "for lr in learning_rate_test:\n",
    "    print(f' ============================= START OF {lr} =============================')    \n",
    "    results = []\n",
    "    for rep in range(n_repetitions):\n",
    "        agent = ReinforceAgent(tf.keras.models.clone_model(model1), lr, 0.9)\n",
    "        exp_stats = agent.train(n_episodes)\n",
    "        results.append(exp_stats['episode_overall_reward'])\n",
    "    # average over repetitions\n",
    "    results_array = np.array(results)\n",
    "    results_averaged = np.mean(results_array, axis=0)\n",
    "    # further smoothing\n",
    "    results_smoothed = smooth(results_averaged,smoothing_window)\n",
    "    plot_label = lr \n",
    "    Plot.add_curve(results_smoothed,label=plot_label)\n",
    "Plot.save('A3_lr_plots')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic agent implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticAgent():\n",
    "\n",
    "    def __init__(self, model, learning_rate, gamma, optimizer, loss_func):\n",
    "        self.model = tf.keras.models.clone_model(model)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_func = loss_func\n",
    "        \n",
    "    def train(self, n_episodes, type_of_algorithm):\n",
    "        action_probabilities_list = []\n",
    "        critic_values_list = []\n",
    "        reward_list = []\n",
    "\n",
    "        experiment_stats = []\n",
    "        \n",
    "        for episode in range(n_episodes):\n",
    "            s = env.reset()\n",
    "            reward_per_episode = 0\n",
    "\n",
    "            done = False\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                while not done:\n",
    "                    # convert state to a tensor of shape = (4,)\n",
    "                    s = tf.convert_to_tensor(s)\n",
    "                    # convert state to a tensor of shape = (1, 4)\n",
    "                    s = tf.expand_dims(s, 0)\n",
    "\n",
    "                    # get predictions probabilities for actions\n",
    "                    action_probabilities = self.model(s)[0]\n",
    "\n",
    "                    # select action based on the above probabilities\n",
    "                    a = np.random.choice(n_actions, p = np.squeeze(action_probabilities))\n",
    "\n",
    "                    # get estimated future reward, i.e. critic prediction\n",
    "                    critic_prediction = self.model(s)[1]\n",
    "\n",
    "                    # save the actions and critic values\n",
    "                    action_probabilities_list.append(tf.math.log(action_probabilities[0, a]))\n",
    "                    critic_values_list.append(critic_prediction[0, 0])\n",
    "\n",
    "                    # simulate the environment\n",
    "                    s_next, r, done, _ = env.step(a)\n",
    "                    reward_per_episode += r\n",
    "                    reward_list.append(r)                      \n",
    "                        \n",
    "                    s = s_next\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                print(f'Episode: {episode}, reward_per_episode: {reward_per_episode}')\n",
    "                \n",
    "                # save the results in a list for plotting\n",
    "                experiment_stats.append(reward_per_episode)\n",
    "\n",
    "                # compute expected value\n",
    "                discounted_sum = 0\n",
    "                returns = []\n",
    "                for r in reward_list[::-1]:\n",
    "                    discounted_sum = r + self.gamma * discounted_sum\n",
    "                    returns.insert(0, discounted_sum)\n",
    "\n",
    "                # normalize the results\n",
    "                returns = np.array(returns)\n",
    "                returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
    "                returns = returns.tolist()\n",
    "                \n",
    "                # apply bootstrapping and baseline subtraction if required:\n",
    "                if type_of_algorithm == 'vanilla':\n",
    "                    # compute loss values for the actor and critic models\n",
    "                    history = zip(action_probabilities_list, critic_values_list, returns)\n",
    "                    \n",
    "                elif type_of_algorithm == 'bootstrap':\n",
    "                    bootstrapped_critic_values_list = []\n",
    "                    \n",
    "                    for i in range(len(critic_values_list)):\n",
    "                        n = len(critic_values_list) - i\n",
    "                        if n>= 10:\n",
    "                            bootstrap_value = returns[i+8] + critic_values_list[i]\n",
    "                        elif n < 10:\n",
    "                            bootstrap_value = returns[n-1] + critic_values_list[i]\n",
    "                            \n",
    "                        bootstrapped_critic_values_list.append(bootstrap_value)\n",
    "                        \n",
    "                    history = zip(action_probabilities_list, bootstrapped_critic_values_list, returns)\n",
    "                    \n",
    "                elif type_of_algorithm == 'baseline_subtraction':\n",
    "                    bs_critic_values_list = []\n",
    "                    \n",
    "                    for i in range(len(critic_values_list)):\n",
    "                        baseline_subtraction_value = returns[i] - critic_values_list[i]\n",
    "                        bs_critic_values_list.append(baseline_subtraction_value)\n",
    "                        \n",
    "                    history = zip(action_probabilities_list, bs_critic_values_list, returns)\n",
    "                    \n",
    "                elif type_of_algorithm == 'bootstrap_baseline':\n",
    "                    bbs_critic_values_list = []\n",
    "                    \n",
    "                    for i in range(len(critic_values_list)):\n",
    "                        n = len(critic_values_list) - i\n",
    "                        if n>= 10:\n",
    "                            bootstrap_value = returns[i+8] + critic_values_list[i]\n",
    "                        elif n < 10:\n",
    "                            bootstrap_value = returns[n-1] + critic_values_list[i]\n",
    "                            \n",
    "                        bbs_value = bootstrap_value - critic_values_list[i]\n",
    "                        bbs_critic_values_list.append(bbs_value)\n",
    "                        \n",
    "                    history = zip(action_probabilities_list, bbs_critic_values_list, returns)\n",
    "                    \n",
    "                actor_l = []\n",
    "                critic_l = []\n",
    "\n",
    "                for log_probability, value, ret in history:\n",
    "                    advantage_function_value = ret - value\n",
    "\n",
    "                    actor_l.append(-log_probability * advantage_function_value)\n",
    "\n",
    "                    loss_c = self.loss_func(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
    "                    critic_l.append(loss_c)\n",
    "\n",
    "                # perform backpropagation\n",
    "                total_loss = sum(actor_l) + sum(critic_l)\n",
    "\n",
    "                grads = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "                self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "                # clear the lists for action prabilities, critic values and reward list\n",
    "                action_probabilities_list.clear()\n",
    "                critic_values_list.clear()\n",
    "                reward_list.clear()\n",
    "                \n",
    "        return experiment_stats        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Actor-critic model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "lr = 0.01\n",
    "# really really really small number\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "env.seed(0)\n",
    "\n",
    "# the first layers are the same for both the actor and the critic model\n",
    "input_layer = tf.keras.layers.Input(shape = (n_states,))\n",
    "intermediary_layer1 = tf.keras.layers.Dense(128, activation = 'relu')(input_layer)\n",
    "\n",
    "# create the output layers for the 2 models\n",
    "action_output_layer = tf.keras.layers.Dense(n_actions, activation = 'softmax')(intermediary_layer1)\n",
    "critic_output_layer = tf.keras.layers.Dense(1)(intermediary_layer1) \n",
    "\n",
    "\n",
    "# create a model variable that works the 2 models in parallel\n",
    "model = tf.keras.Model(inputs = input_layer, outputs = [action_output_layer, critic_output_layer])\n",
    "\n",
    "# create an optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
    "# create a loss function\n",
    "huber_loss = tf.keras.losses.Huber()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_over_repetitions(reps, model, learning_rate, gamma, optimizer, loss_func, n_episodes, type_of_algorithm):\n",
    "    m = tf.keras.models.clone_model(model)\n",
    "    agent = ActorCriticAgent(m, learning_rate, gamma, optimizer, loss_func)\n",
    "    reward_results = np.empty([reps, n_episodes]) # Result array\n",
    "    now = time.time()\n",
    "    \n",
    "    for rep in range(reps): # Loop over repetitions\n",
    "        print(\"----------------------- Repetition \" + str(rep) + \" -----------------------\")\n",
    "        reward_results[rep] = agent.train(n_episodes, type_of_algorithm)       \n",
    "          \n",
    "    learning_curve = np.mean(reward_results,axis=0) # average over repetitions\n",
    "    learning_curve = smooth(learning_curve,5) # additional smoothing\n",
    "    return learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Several Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: one hidden layer of medium size\n",
    "input_layer1 = tf.keras.layers.Input(shape = (n_states,))\n",
    "intermediary_layer1 = tf.keras.layers.Dense(128, activation = 'relu')(input_layer1)\n",
    "action_output_layer1 = tf.keras.layers.Dense(n_actions, activation = 'softmax')(intermediary_layer1)\n",
    "critic_output_layer1 = tf.keras.layers.Dense(1)(intermediary_layer1)\n",
    "model1 = tf.keras.Model(inputs = input_layer1, outputs = [action_output_layer1, critic_output_layer1])\n",
    "\n",
    "# Model 2: two hidden layers of 128 neurons\n",
    "input_layer2 = tf.keras.layers.Input(shape = (n_states,))\n",
    "intermediary_layer21 = tf.keras.layers.Dense(128, activation = 'relu')(input_layer2)\n",
    "intermediary_layer22 = tf.keras.layers.Dense(128, activation = 'relu')(intermediary_layer21)\n",
    "action_output_layer2 = tf.keras.layers.Dense(n_actions, activation = 'softmax')(intermediary_layer22)\n",
    "critic_output_layer2 = tf.keras.layers.Dense(1)(intermediary_layer22)\n",
    "model2 = tf.keras.Model(inputs = input_layer2, outputs = [action_output_layer2, critic_output_layer2])\n",
    "\n",
    "# Model 3: one hidden layer of 128 neurons\n",
    "input_layer3 = tf.keras.layers.Input(shape = (n_states,))\n",
    "intermediary_layer3 = tf.keras.layers.Dense(128, activation = 'relu')(input_layer3)\n",
    "action_output_layer3 = tf.keras.layers.Dense(n_actions, activation = 'softmax')(intermediary_layer3)\n",
    "critic_output_layer3 = tf.keras.layers.Dense(1)(intermediary_layer3)\n",
    "model3 = tf.keras.Model(inputs = input_layer3, outputs = [action_output_layer3, critic_output_layer3])\n",
    "\n",
    "# Model 4: one hidden layer of 16 neurons\n",
    "input_layer4 = tf.keras.layers.Input(shape = (n_states,))\n",
    "intermediary_layer4 = tf.keras.layers.Dense(16, activation = 'relu')(input_layer4)\n",
    "action_output_layer4 = tf.keras.layers.Dense(n_actions, activation = 'softmax')(intermediary_layer4)\n",
    "critic_output_layer4 = tf.keras.layers.Dense(1)(intermediary_layer4)\n",
    "model4 = tf.keras.Model(inputs = input_layer4, outputs = [action_output_layer4, critic_output_layer4])\n",
    "\n",
    "# Model 5: one hidden layer of 4 neurons\n",
    "input_layer5 = tf.keras.layers.Input(shape = (n_states,))\n",
    "intermediary_layer5 = tf.keras.layers.Dense(4, activation = 'relu')(input_layer5)\n",
    "action_output_layer5 = tf.keras.layers.Dense(n_actions, activation = 'softmax')(intermediary_layer5)\n",
    "critic_output_layer5 = tf.keras.layers.Dense(1)(intermediary_layer5)\n",
    "model5 = tf.keras.Model(inputs = input_layer5, outputs = [action_output_layer5, critic_output_layer5])\n",
    "\n",
    "# Model 6: two hidden layers of 64 neurons\n",
    "input_layer6 = tf.keras.layers.Input(shape = (n_states,))\n",
    "intermediary_layer61 = tf.keras.layers.Dense(64, activation = 'relu')(input_layer6)\n",
    "intermediary_layer62 = tf.keras.layers.Dense(64, activation = 'relu')(intermediary_layer61)\n",
    "action_output_layer6 = tf.keras.layers.Dense(n_actions, activation = 'softmax')(intermediary_layer62)\n",
    "critic_output_layer6 = tf.keras.layers.Dense(1)(intermediary_layer62)\n",
    "model6 = tf.keras.Model(inputs = input_layer6, outputs = [action_output_layer6, critic_output_layer6])\n",
    "\n",
    "# Model 7: two hidden layers of 256 neurons\n",
    "input_layer7 = tf.keras.layers.Input(shape = (n_states,))\n",
    "intermediary_layer71 = tf.keras.layers.Dense(256, activation = 'relu')(input_layer7)\n",
    "intermediary_layer72 = tf.keras.layers.Dense(256, activation = 'relu')(intermediary_layer71)\n",
    "action_output_layer7 = tf.keras.layers.Dense(n_actions, activation = 'softmax')(intermediary_layer72)\n",
    "critic_output_layer7 = tf.keras.layers.Dense(1)(intermediary_layer72)\n",
    "model7 = tf.keras.Model(inputs = input_layer7, outputs = [action_output_layer7, critic_output_layer7])\n",
    "\n",
    "# Model 8: three hidden layers of 128 neurons\n",
    "input_layer8 = tf.keras.layers.Input(shape = (n_states,))\n",
    "intermediary_layer81 = tf.keras.layers.Dense(256, activation = 'relu')(input_layer8)\n",
    "intermediary_layer82 = tf.keras.layers.Dense(256, activation = 'relu')(intermediary_layer81)\n",
    "intermediary_layer83 = tf.keras.layers.Dense(256, activation = 'relu')(intermediary_layer82)\n",
    "action_output_layer8 = tf.keras.layers.Dense(n_actions, activation = 'softmax')(intermediary_layer83)\n",
    "critic_output_layer8 = tf.keras.layers.Dense(1)(intermediary_layer83)\n",
    "model8 = tf.keras.Model(inputs = input_layer8, outputs = [action_output_layer8, critic_output_layer8])\n",
    "\n",
    "models = {'model1': model1, 'model2': model2, 'model3': model3, 'model4': model4, 'model5': model5, 'model6': model6,\n",
    "          'model7': model7, 'model8': model8}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with different architectures and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 5\n",
    "n_episodes = 250\n",
    "gamma = 0.9\n",
    "learning_rate = 0.01\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
    "huber_loss = tf.keras.losses.Huber()\n",
    "type_of_algorithm = 'bootstrap_baseline'\n",
    "\n",
    "Plot = LearningCurvePlot(title = 'Effect of various network architectures')\n",
    "for key in models.keys():\n",
    "    print(\"########## Running for \" + key + \" ##########\")\n",
    "    learning_curve = average_over_repetitions(reps, models[key], learning_rate, gamma, optimizer, huber_loss, n_episodes,\n",
    "                                             type_of_algorithm)\n",
    "    Plot.add_curve(learning_curve, label=key)\n",
    "        \n",
    "Plot.save('./architecture_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with the Learning Rates and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 5\n",
    "n_episodes = 250\n",
    "gamma = 0.9\n",
    "lr_values = [0.001, 0.01, 0.1]\n",
    "huber_loss = tf.keras.losses.Huber()\n",
    "type_of_algorithm = 'bootstrap_baseline'\n",
    "model = model5 # change after running architecture experiment\n",
    "\n",
    "Plot = LearningCurvePlot(title = 'Effect of various learning rates')\n",
    "for lr in lr_values:\n",
    "    print(\"########## Running for learning rate \" + str(lr) + \" ##########\")\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "    learning_curve = average_over_repetitions(reps, model, lr, gamma, optimizer, huber_loss, n_episodes,\n",
    "                                             type_of_algorithm)\n",
    "    Plot.add_curve(learning_curve, label=\"lr = \" + str(lr))\n",
    "        \n",
    "Plot.save('./lr_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with different discount factors (gamma values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 5\n",
    "n_episodes = 250\n",
    "gamma_values = [0.3, 0.6, 0.9]\n",
    "learning_rate = 0.001 # change after running lr experiment\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "huber_loss = tf.keras.losses.Huber()\n",
    "type_of_algorithm = 'bootstrap_baseline'\n",
    "model = model5 # change after running architecture experiment\n",
    "\n",
    "Plot = LearningCurvePlot(title = 'Effect of various gamma values')\n",
    "for gamma in gamma_values:\n",
    "    print(\"########## Running for gamma \" + str(gamma) + \" ##########\")\n",
    "    learning_curve = average_over_repetitions(reps, model, learning_rate, gamma, optimizer, huber_loss, n_episodes,\n",
    "                                             type_of_algorithm)\n",
    "    Plot.add_curve(learning_curve, label=\"gamma = \" + str(gamma))\n",
    "        \n",
    "Plot.save('./gamma_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with Different Versions\n",
    "\n",
    "- Vanilla Actor-Critic\n",
    "- Bootstrap Actor-Critic\n",
    "- Baseline subtraction Actor-Critic\n",
    "- Bootstrap + baseline subtraction Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 5\n",
    "n_episodes = 250\n",
    "gamma = 0.9\n",
    "learning_rate = 0.001 # change after running lr experiment\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "huber_loss = tf.keras.losses.Huber()\n",
    "type_of_algorithm = 'bootstrap_baseline'\n",
    "model = model5 # change after running architecture experiment\n",
    "types = ['vanilla', 'bootstrap', 'baseline_subtraction', 'bootstrap_baseline']\n",
    "\n",
    "Plot = LearningCurvePlot(title = 'Comparison of the four different modes')\n",
    "for t in types:\n",
    "    print(\"########## Running for mode \" + t + \" ##########\")\n",
    "    learning_curve = average_over_repetitions(reps, model, learning_rate, gamma, optimizer, huber_loss, n_episodes, t)\n",
    "    Plot.add_curve(learning_curve, label=t)\n",
    "        \n",
    "Plot.save('./type_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare REINFORCE vs. Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REINFORCE\n",
    "# Model with no hidden layers\n",
    "model1_re = tf.keras.Sequential()\n",
    "model1_re.add(tf.keras.layers.Dense(n_actions, activation='softmax', input_shape=(None,n_states)))\n",
    "\n",
    "# Actor-Critic\n",
    "# Model 5: one hidden layer of 4 neurons\n",
    "input_layer5 = tf.keras.layers.Input(shape = (n_states,))\n",
    "intermediary_layer5 = tf.keras.layers.Dense(4, activation = 'relu')(input_layer5)\n",
    "action_output_layer5 = tf.keras.layers.Dense(n_actions, activation = 'softmax')(intermediary_layer5)\n",
    "critic_output_layer5 = tf.keras.layers.Dense(1)(intermediary_layer5)\n",
    "model5_ac = tf.keras.Model(inputs = input_layer5, outputs = [action_output_layer5, critic_output_layer5])\n",
    "\n",
    "\n",
    "plot_title = 'Comparison between REINFORCE and Actor-Critic' \n",
    "Plot = LearningCurvePlot(title = plot_title)\n",
    "\n",
    "n_repetitions = 5\n",
    "smoothing_window = 5\n",
    "n_episodes = 500\n",
    "gamma = 0.9\n",
    "lr = 0.001\n",
    "\n",
    "model_re = model1_re # Replace with best model for reinforce!!!!!!!!!!!\n",
    "model_ac = model5_ac \n",
    "\n",
    "print(f' ============================= START OF REINFORCE =============================')\n",
    "current_model = model_re\n",
    "    \n",
    "results = []\n",
    "\n",
    "for rep in range(n_repetitions):\n",
    "    print(\"---------------------------- Repetition \" + str(rep) + \" ----------------------------\")\n",
    "    agent = ReinforceAgent(tf.keras.models.clone_model(current_model), lr, gamma)\n",
    "    exp_stats = agent.train(n_episodes)\n",
    "    results.append(exp_stats['episode_overall_reward'])\n",
    "\n",
    "# average over repetitions\n",
    "results_array = np.array(results)\n",
    "results_averaged = np.mean(results_array, axis=0)\n",
    "\n",
    "# further smoothing\n",
    "results_smoothed = smooth(results_averaged,smoothing_window)\n",
    "\n",
    "plot_label = \"REINFORCE\" \n",
    "Plot.add_curve(results_smoothed,label=plot_label)\n",
    "\n",
    "print(f' ============================= START OF Actor-Critic =============================')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
    "# create a loss function\n",
    "huber_loss = tf.keras.losses.Huber()\n",
    "#results.append(exp_stats['episode_overall_reward'])\n",
    "learning_curve = average_over_repetitions(n_repetitions, model_ac, 0.01, gamma, optimizer, huber_loss, n_episodes,\n",
    "                                          'bootstrap_baseline')\n",
    "Plot.add_curve(learning_curve, label='Actor-Critic')\n",
    "\n",
    "Plot.save('./comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy_of_reinforce (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
